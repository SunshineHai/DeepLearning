{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d935d5f7",
   "metadata": {},
   "source": [
    "\n",
    "# 神经网实现      手写识别体的多分类\n",
    "\n",
    "\n",
    "上篇博文，忽略偏置(b)，且代价(损失)函数直接就是差的平方和，本篇文章增加了对偏置的讨论，本文是本人的学习笔记，如果出现问题，欢迎大家批评指正。\n",
    "\n",
    "需具备知识：\n",
    "\n",
    "- 二元函数的偏导数的求解和意义\n",
    "- 链式法则求导\n",
    "\n",
    "\n",
    "## 数据集的下载\n",
    "\n",
    "使用以下git命令克隆：\n",
    "\n",
    "\n",
    "```\n",
    "git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git\n",
    "```\n",
    "\n",
    "该数据集分为：\n",
    "\n",
    "- 60000 幅训练图像\n",
    "- 10000 幅测试图像\n",
    "\n",
    "我们把 60000幅训练图像再划分为：\n",
    "- 50000 幅训练集\n",
    "- 10000 幅验证集\n",
    "\n",
    "使用验证集有助于我们设置神经网络的某些**超参数**，例如学习率（lr）等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735d06d2",
   "metadata": {},
   "source": [
    "### 定义神经网络代码的核心片段"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d44c8a",
   "metadata": {},
   "source": [
    "假如定义最简单的3层神经网络，```sizes = [2, 3, 1]```,则：\n",
    "\n",
    "偏置的维度：(3, 1) <br/>\n",
    "权重的维度：(3, 2), (1, 3)\n",
    "\n",
    "以下定义的 类 Network 中的偏置bias 和权重weights 都是随机变化的，值为0~1之间的正态分布数。\n",
    "\n",
    "假如我们实例化对象：\n",
    "```python\n",
    "    net = Network()\n",
    "```\n",
    "\n",
    "**注：**\n",
    "-  np.random.randn() 生成的是均值为0，标准差为1的高斯分布(正态分布)。\n",
    "-  第一层神经元是一个输入层，不对其设置任何偏置，因为偏置在后面的层中用于计算输出。\n",
    "-  偏置和矩阵以 NumPy 矩阵列表的形式存储。例如：net.weights[1] : 第2层和第3层神经元之间的权重。\n",
    "\n",
    "我们使用 $ \\omega $ 表示矩阵， $ \\omega_{jk} $ 表示第2层的k个神经元，第3层的j个神经元的权重。\n",
    "\n",
    "这样写有点绕，目的是为了计算 权重 和 输入层变量进行 点乘，即：\n",
    "\n",
    "$$\n",
    "    a^{\\prime} = \\sigma(\\omega \\cdot a + b)\n",
    "$$\n",
    "\n",
    "以上例子， $\\omega$ 是 第2、3层的权重激活向量， a 是第二次神经元的**激活向量**(可以理解为权重\\*输入+偏置之后经过激活函数)，\n",
    "\n",
    "$ \\sigma() $ 函数是 **激活函数**sigmoid函数，定义如下：\n",
    "\n",
    "$$\n",
    "    sigmoid = \\frac{1}{1+e^{-x}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822e068f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))  \n",
    "\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefd00ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Network at 0x222a079bf70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Network(object):\n",
    "    '''\n",
    "        sizes ： 各层神经元的个数\n",
    "    '''\n",
    "    def __init__(self, sizes:list):\n",
    "        \n",
    "        self.num_layers = len(sizes)                                 # 神经元的层数\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]      # 偏置 b\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "    pass\n",
    "\n",
    "Network([2, 3, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "775d61cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.42305446],\n",
       "        [0.09270809],\n",
       "        [1.37065137]]),\n",
       " array([[-0.60325518]])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = [2, 3, 1]\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "biases   # 偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a75bcde4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.85328277, -0.65275394],\n",
       "        [-1.23824422, -0.83507403],\n",
       "        [-0.36639638, -1.57123581]]),\n",
       " array([[-0.11760388, -0.96681311,  0.49207488]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] # 权重\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b85fdb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[0.]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.zeros(b.shape) for b in biases]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfdd4e8",
   "metadata": {},
   "source": [
    "### 前向传播\n",
    "\n",
    "该方法和我上一个博文中mnist的多分类预测中，写的 query() 是相同的功能实现。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "161b7897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward(self, a):\n",
    "    \"\"\"若 a 为输入，则返回输出\n",
    "        注：a 为 上一个节点的激活输出，如果为第一个节点，则是输入层的神经元个数，维度为 (n_variable, 1) 的NumPy ndarray数组，而不是\n",
    "        (n_variable,)的向量，这样写的目的是便于计算机计算，也便于修改代码实现同时前馈多个输入。\n",
    "        并且，CPU 一次只传入一个样本进行训练，右边的 1 代表一个样本。\n",
    "        \n",
    "    \"\"\"\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        a = sigmoid(np.dot(w, a) + self.biases)  # z = w*x+b  a = sigmoid(z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4085da5e",
   "metadata": {},
   "source": [
    "### 梯度下降算法\n",
    "\n",
    "\n",
    "#### 随机梯度下降算法\n",
    "\n",
    "这里我们使用**随机梯度下降算法**\n",
    "\n",
    "#### 代价函数\n",
    "\n",
    "公式如下：\n",
    "\n",
    "$$\n",
    "    z = w \\cdot x + b \\tag{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    a = \\sigma(z) \\tag{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    C(w, b) \\equiv \\frac{1}{2n}\\sum_x \\left||y(x)-a\\right||^2 \\tag{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    \\sigma(z) = \\frac{1}{1+e^{-z}} \\tag{4}\n",
    "$$\n",
    "\n",
    "sigma 函数 是激活函数的一种，本文激活函数选择的是sigmoid函数，Python实现如下：\n",
    "\n",
    "```\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0+np.exp(-x))\n",
    "```\n",
    "\n",
    "公式解释： \n",
    "- 公式1中 w 是**权重**， b是**偏置**，x 表示的是样本，一般维度为(n_varible, 1)，如果是CPU一次训练1个样本，z是激活函数的输入；\n",
    "\n",
    "注意，这里 x 是一个训练样本，不是变量，w, b 才是变量。\n",
    "    \n",
    "     \n",
    "- 公式2中， 激活函数的输入是 公式1中的结果 z, 通过激活函数得到的结果是 a。\n",
    "\n",
    "注意：a(z) = sigmoid(z) a 是 z 的函数，z 是 w、x的函数，进而，a也是 w、x的函数。\n",
    "\n",
    "- 公式3， 叫做 **代价函数**，也叫*损失函数*或者目标函数，本文叫该函数为代价函数。该公式称为\n",
    "\n",
    "**二次代价函数** 也称 **均方误差(MSE)**， y(x) 表示 x (x是常量)样本的真实值， $a$ 表示 a 表示 通过权重、偏置、激活函数之后的输\n",
    "\n",
    "出，如果是最后一层，即是预测值，如果是中间层即是每个神经元通过激活函数的输出值。\n",
    "\n",
    "- 公式4是 sigmoid 函数，比较简单，不再解释。\n",
    "\n",
    "\n",
    "刚开始训练时，误差比较大，即 y(x) 和 $a$ 相差较大， 我们的目的是最小化 代价函数 C(w, b),换句话说：我们想找到 让代价函数尽可能小的\n",
    "\n",
    "权重和偏置，这该怎么实现哪？\n",
    "\n",
    "我们往下看：\n",
    "\n",
    "我们定义\n",
    "$$\n",
    "       \\nabla C = \\big(\\frac{\\partial C}{\\partial w}, \\frac{\\partial C}{\\partial b} \\big)^T \\tag{5}\n",
    "$$    \n",
    "\n",
    "$$\n",
    "    w^{\\prime} = w - \\eta \\frac{\\partial C}{\\partial w}  \\tag{6}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    b^{\\prime} = b - \\eta \\frac{\\partial C}{\\partial b}  \\tag{7}\n",
    "$$\n",
    "\n",
    "假如 $v=(w, b)$ 表示变量 v 有2个变量，分别是 w, b。则\n",
    "\n",
    "$$\n",
    "  v^{\\prime} = v - \\eta \\nabla C \\tag{8}\n",
    "$$\n",
    "\n",
    "解释：\n",
    "- 公式5，只是把 C 对变量 w 和 b的偏导数 变成一个矩阵形式， **偏导数**$\\Delta w = \\frac{\\partial C}{\\partial w} \\Delta b = \\frac{\\partial C}{\\partial b}$\n",
    "\n",
    "**表示C相对于w的变化率**。\n",
    "\n",
    "- 公式6，**梯度下降公式**， 左边表示新的权重值，右边表示 旧的权重值 - 学习率 乘 **变化率**, $\\eta$ 表示**学习率**。\n",
    "\n",
    "字母可以随便改变，不同书籍使用的不一样，只要能够理解就可以。\n",
    "\n",
    "问题：如果找到 合适的**权重和偏置** 使得 **代价函数** 最小？？？\n",
    "\n",
    "我们使用 **梯度下降算法和反向传播算法**来解决这个问题，其中，梯度下降算法是为了找到代价函数的最小值，使用梯度下降公式，一步一步更新\n",
    "\n",
    "权重和偏置，最重要的就是求代价函数的**偏导数**, 这是我们使用 **反向传播算法** 从最后一层神经元的**激活向量**$a(z(w, b))$，依次求\n",
    "\n",
    "出前一层的神经元的**误差**, 因为更新 每一层的 权重和偏置时，需要用到 每一层的激活向量值$a(z(w, b))$。公式如下：\n",
    "\n",
    "- 反向传播:对每个l = L-1,L-2,...2\n",
    "$$\n",
    "    \\delta^{x, l} = ((w_{l+1}^T\\delta^{x, l+1}) * \\sigma^{\\partial}(z^{x,l}) )  \\tag{9}   \n",
    "$$\n",
    "\n",
    "- 梯度下降：对每个l = L, L-1, L-2, ..., 2,根据\n",
    "\n",
    "$$\n",
    "    w^{l} -> \\frac{\\eta}{m} \\sum  _{x} \\delta^{x, l}(a^{x,l-1})^T \\tag{10}\n",
    "$$\n",
    "\n",
    "\n",
    "和 \n",
    "\n",
    "$$\n",
    "    b^{l} -> b^l - \\frac{\\eta}{m}\\sum_x \\delta^{x, l} \\tag{11}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39612009",
   "metadata": {},
   "source": [
    "#### 疑问：什么是梯度？什么是误差？什么是偏导数？\n",
    "\n",
    "以下公式直接是推导出来的答案，如果感兴趣的话，具体如何推导的可以查看相关机器学习类的书籍或者网上的资料。\n",
    "\n",
    "\n",
    "误差的定义：\n",
    "    $$\n",
    "        \\delta = \\frac{\\partial C}{\\partial z}\n",
    "    $$\n",
    "    \n",
    "注：z 表示激活函数的输入。\n",
    "\n",
    "推论：\n",
    "\n",
    "- 最后一层误差的求解：\n",
    "\n",
    "$$\n",
    "    \\delta = \\frac{\\partial C}{\\partial a} * \\partial^{'}(z) = (W_{后}^T \\cdot \\delta) * \\partial^{'}(z)\n",
    "$$\n",
    "    \n",
    "- C对b的偏导数：\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C}{\\partial b} = \\delta\n",
    "$$\n",
    "\n",
    "- C 对 w 的偏导数：\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial C}{\\partial w} = \\delta * a_{前}^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b943f9c",
   "metadata": {},
   "source": [
    "#### 梯度下降算法的实现：\n",
    "\n",
    "对于每个训练样本：\n",
    "\n",
    "- 前向传播\n",
    "- 输出误差 : \n",
    "        \n",
    " 最后一层 的误差求解为 (a - y(x)) *sigmoid的导数 。\n",
    " \n",
    "- 反向传播误差\n",
    "\n",
    " 倒数第二层的误差，使用最后一层的误差求解,依次类推。\n",
    "\n",
    "梯度下降：\n",
    "\n",
    "$$\n",
    "    w_{new} = w_{old} - \\frac{\\eta}{m} \\frac{\\partial C}{\\partial w}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    b_{new} = b_{old} - \\frac{\\eta}{m} \\frac{\\partial C}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b96be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降：\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\" SGD : stochastic gradient descent\n",
    "            training_data : 训练集，传入 zip() 对象， 使用 list() 后数据变为：[((784, 1), (10, 1)), ...]\n",
    "            epochs : 训练轮数\n",
    "            mini_batch_size : 批大小，每次训练的小样本批次大小\n",
    "            eta : 学习率\n",
    "            test_data : 可选参数， 对于追踪训练结果是有用的，但是会拖慢训练速度\n",
    "        \"\"\"\n",
    "        training_data = list(training_data)       # 把 zip() 对象 ==> list [((784, 1), (10, 1)), ...]\n",
    "        n = len(training_data)                    # 训练集样本个数\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)               # 测试集样本的长度\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)         # 打乱(训练集)列表元素顺序\n",
    "            # [[((784, 1), (10, 1)), ..., ], [mini_batch_size个],...]\n",
    "            mini_batchs = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batchs:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "\n",
    "            if test_data:\n",
    "                print(f\"Epoch {j}:{self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} completed.\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebce3391",
   "metadata": {},
   "source": [
    "#### 反向传播代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdefdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "            x : (784, 1) , ndarray 对象\n",
    "            y : (10, 1) , ndarray 对象\n",
    "            返回元组(nabla_b, nable_w) 代表成本/代价/损失函数 C_x 的 梯度(偏C/偏b, 偏C/偏w)。\n",
    "            ([(30, 1), (10, 1)], [(30, 784), (10, 30)])\n",
    "\n",
    "            前面注释可知公式：\n",
    "            输出误差  delta = 偏C/偏a * sigma 的导数\n",
    "            偏C/偏w = 误差delta · 前一层的激活层 a 的转秩\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]  # [(30， 1), (10， 1)]     存放偏置的误差 偏C/偏b\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]  # [(30, 784), [10, 30]]   存放权重的误差 偏C/偏w\n",
    "\n",
    "        # 前向传播\n",
    "        activation = x      # 激活值\n",
    "        activations = [x]   # [(784, 1)] , list 列表 ： 存放所有的激活值,其中 a = sigmoid(z)\n",
    "\n",
    "        zs = []  # list 存放每层的 z 向量（激活函数的输入值）， 其中 z = w · x + b [(30, 1), (10, 1)]\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward 反向传播\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1]) # 求最后一层的误差 delta = 偏C/偏a * sigma 的导数\n",
    "        nabla_b[-1] = delta                          # 偏C/偏b\n",
    "        # print(f\"C/w:{delta.shape}, a前.shape:{activations[-2].shape}\", )\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())  # 偏C/偏w = 误差delta · 前一层的激活层 a 的转秩\n",
    "\n",
    "        \"\"\"\n",
    "            前面已经求出了最后一层的 偏C/偏b 和 偏C/偏w\n",
    "            从倒数第二层开始往前遍历到 正数第二层\n",
    "        \"\"\"\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]              # 第一次遍历时，l = 2 ,表示倒数第二层 的激活函数的输入 z 向量\n",
    "            sp = sigmoid_prime(z)   # 激活函数的倒数\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp     # delta = 后一层的w · 后一层的delta * 当前层的激活函数的倒数\n",
    "            nabla_b[-l] = delta        # 误差、梯度、偏导数\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())     # 偏C/偏w = 误差delta · 前一层的激活层 a 的转秩\n",
    "        return (nabla_b, nabla_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e961fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "training_data = np.arange(6).reshape(6,)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff57d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_data)\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26136ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [1, 2, 3, 4, 5]\n",
    "random.shuffle(seq)\n",
    "seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a1876",
   "metadata": {},
   "source": [
    "## 数据集的处理\n",
    "\n",
    "- 解压 .gz 的压缩包\n",
    "- 把数据集分为训练集、验证集、测试集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c457d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .gz 的压缩包 使用 gzip() 函数进行解压\n",
    "\n",
    "import gzip\n",
    "import pickle  # 块 pickle 实现了对一个 Python 对象结构的二进制序列化和反序列化\n",
    "\n",
    "f = gzip.open(r'..\\mnist.pkl.gz')         # 返回 gzip.GzipFile 对象\n",
    "\n",
    "# training_data : ((50000, 784), (50000,)) 元组：2个元素，都是 NumPy对象， 维度如前所示\n",
    "# validation_data : ((10000, 784), (10000,))\n",
    "# test_data : ((10000, 784), (10000,))\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1') # 从已打开的 file object 文件 中读取封存后的对象，重建其中特定对象的层次结构并返回。\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfa540e3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((784, 1), (784, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 调整 training_data 的格式/维度， 方便训练\n",
    "import numpy as np\n",
    "\n",
    "# （5000， 784） 每一条(784,) 训练数据 转换成 (784, 1)的二维数据，存放到 list 中。\n",
    "training_input = [np.reshape(x, (784, 1)) for x in training_data[0]]  \n",
    "training_input[0].shape, training_input[1].shape\n",
    "\n",
    "# data = np.arange(50).reshape(10, 5)\n",
    "# data = data[:2, :]\n",
    "# [np.reshape(x, (5, 1)) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db9440b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " array([[1.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorized_result(label):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[label] = 1.0\n",
    "    return e\n",
    "\n",
    "\n",
    "training_data[0].shape, training_data[1].shape # ((50000, 784), (50000,))\n",
    "# 我们要把 training_data[1] 的每个标签 转化为神经网络期望的输出 每个标签都是(10, 1)的ndarray数组\n",
    "print(training_data[1][:2])\n",
    "[vectorized_result(x) for x in training_data[1][:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6712ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = [np.reshape(x, (784, 1)) for x in training_data[0][:2, :]]\n",
    "training_results = [vectorized_result(y) for y in training_data[1][:2]]\n",
    "\n",
    "training_data = zip(training_inputs, training_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6f77e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zip"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eea75e60",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for X, y in training_data:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9c79fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 6, 9]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in range(0, 10, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d65de90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3, 4 ,5]\n",
    "a[2:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4fb0cd4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([0, 10, 2, 6])\n",
    "a\n",
    "np.argmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0a84d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([1, 8, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7249912",
   "metadata": {},
   "source": [
    "$\\delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4978fe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fef77cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_batch_size = 5\n",
    "training_data = [i for i in range(20)]\n",
    "n = len(training_data)\n",
    "print(training_data)\n",
    "mini_batch_size = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "mini_batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f54dee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(10, 1)\n",
    "# [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3343e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15858945],\n",
       "       [0.43174131],\n",
       "       [0.67859285],\n",
       "       [0.8891154 ],\n",
       "       [0.50479713],\n",
       "       [0.24900165],\n",
       "       [0.5631615 ],\n",
       "       [0.62361639],\n",
       "       [0.17701574],\n",
       "       [0.50959991]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "846cd0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15858945, 0.43174131, 0.67859285, 0.8891154 , 0.50479713,\n",
       "        0.24900165, 0.5631615 , 0.62361639, 0.17701574, 0.50959991]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.transpose() # 转秩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e0559625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(i)\n",
    "    \n",
    "i += 10\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fe27e8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+1 * \\\n",
    "10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "264668c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 所有代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaeea3b",
   "metadata": {},
   "source": [
    "network.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457ec4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "\n",
    "class Network(object):\n",
    "    '''\n",
    "        sizes ： 各层神经元的个数\n",
    "    '''\n",
    "    def __init__(self, sizes :list):\n",
    "\n",
    "        self.num_layers = len(sizes)                                  # 神经元的层数\n",
    "        self.biases = [np.random.randn(y, 1)\n",
    "                       for y in sizes[1:]]      # 偏置 b\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]  # 权重\n",
    "\n",
    "    # 前馈传播\n",
    "    def feedforward(self, a):\n",
    "        \"\"\"\n",
    "            如果 a 是输入，则返回神经网络的输出。\n",
    "            假如初始化为 3层的神经网络：[784, 30, 10]， 则：\n",
    "            w = [(30, 784), (10, 30)]\n",
    "            b = [(30, 1), (10, 1)]\n",
    "            第一层时， a 的输入为 (784, 1), a 即是 激活层：通过激活函数得到的值，第一层时为样本输入。\n",
    "            计算：\n",
    "            第2层 : w · a + b :  (30, 784) · (784, 1) + (30, 1) ==> (30, 1)\n",
    "            第3层 : w · a + b :  (10, 30) · (30, 1) + (10, 1)   ==> (10, 1)\n",
    "        \"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)       # z = w · x + b   a = sigmoid(a)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\" SGD : stochastic gradient descent\n",
    "            training_data : 训练集，传入 zip() 对象， 使用 list() 后数据变为：[((784, 1), (10, 1)), ...]\n",
    "            epochs : 训练轮数\n",
    "            mini_batch_size : 批大小，每次训练的小样本批次大小\n",
    "            eta : 学习率\n",
    "            test_data : 可选参数， 对于追踪训练结果是有用的，但是会拖慢训练速度\n",
    "        \"\"\"\n",
    "        training_data = list(training_data)       # 把 zip() 对象 ==> list [((784, 1), (10, 1)), ...]\n",
    "        n = len(training_data)                    # 训练集样本个数\n",
    "\n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)               # 测试集样本的长度\n",
    "\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)         # 打乱(训练集)列表元素顺序\n",
    "            # [[((784, 1), (10, 1)), ..., ], [mini_batch_size个],...]\n",
    "            mini_batchs = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batchs:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "\n",
    "            if test_data:\n",
    "                print(f\"Epoch {j}:{self.evaluate(test_data)} / {n_test}\")\n",
    "            else:\n",
    "                print(f\"Epoch {j} completed.\")\n",
    "        pass\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"使用梯度下降法和反向传播更新 mini_batch 的权重和偏置。\n",
    "        mini_batch 类型如下： [((784, 1), (10, 1)), ...] 有 mini_batch_size 个样本(元组)\n",
    "        eta : 学习率\n",
    "        这两个算法比较复杂，梯度下降算法和反向传播算法的使用都是根据理论推导出的公式实现的，因此，我们只需要记住最后推导得出的公式进行实现即可。\n",
    "        反向传步骤：\n",
    "        对于每个样本 x：设置对应的激活值 a, 执行步骤如下：\n",
    "            1.前向传播(feedforward): z = w · x + b , a = sigma(z)\n",
    "            2.求输出误差: 从最后一层开始算起， 输出误差  delta = 偏C/偏a * sigma 的导数， 偏C/偏w = 前一层的a · 当前层的delta\n",
    "            3.反向传播误差 ：从倒数第二层开始，到 正数第二场为止， 计算\n",
    "                误差 倒数第二层的delta = w(最后后一层) · delta(最后一层的) * sigmoid的导数\n",
    "\n",
    "        梯度下降步骤：对每个神经网络层， w_new = w_old - eta · delta · a^T\n",
    "                                   b_new = b_old - eta * delta\n",
    "        神经元成熟 和 每层神经元个数 ：[784, 30, 10]\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]    # [(30， 1), (10， 1)]\n",
    "        nable_w = [np.zeros(w.shape) for w in self.weights]   # [(30, 784), [10, 30]]\n",
    "        for x, y in mini_batch:\n",
    "            # delta_nabla_b : (10, 1), delta_nable_w : (10, 30)\n",
    "            delta_nabla_b, delta_nable_w = self.backprop(x, y)              # 反向传播由最后一层得到当前层的 偏置和权重的误差\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]   # b+delta b\n",
    "            nable_w = [nw + dnw for nw, dnw in zip(nable_w, delta_nable_w)] # w + delta w [(30, 784), (10, 30)]\n",
    "        self.weights = [w - (eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nable_w)] # w = w - eta/m * 偏C/偏w\n",
    "        self.biases = [b - (eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]   # b = b-eta/m * 偏C/偏b\n",
    "        pass\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "        \"\"\"\n",
    "            x : (784, 1) , ndarray 对象\n",
    "            y : (10, 1) , ndarray 对象\n",
    "            返回元组(nabla_b, nable_w) 代表成本/代价/损失函数 C_x 的 梯度(偏C/偏b, 偏C/偏w)。\n",
    "            ([(30, 1), (10, 1)], [(30, 784), (10, 30)])\n",
    "\n",
    "            前面注释可知公式：\n",
    "            输出误差  delta = 偏C/偏a * sigma 的导数\n",
    "            偏C/偏w = 误差delta · 前一层的激活层 a 的转秩\n",
    "        \"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]  # [(30， 1), (10， 1)]     存放偏置的误差 偏C/偏b\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]  # [(30, 784), [10, 30]]   存放权重的误差 偏C/偏w\n",
    "\n",
    "        # 前向传播\n",
    "        activation = x      # 激活值\n",
    "        activations = [x]   # [(784, 1)] , list 列表 ： 存放所有的激活值,其中 a = sigmoid(z)\n",
    "\n",
    "        zs = []  # list 存放每层的 z 向量（激活函数的输入值）， 其中 z = w · x + b [(30, 1), (10, 1)]\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward 反向传播\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1]) # 求最后一层的误差 delta = 偏C/偏a * sigma 的导数\n",
    "        nabla_b[-1] = delta                          # 偏C/偏b\n",
    "        # print(f\"C/w:{delta.shape}, a前.shape:{activations[-2].shape}\", )\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())  # 偏C/偏w = 误差delta · 前一层的激活层 a 的转秩\n",
    "\n",
    "        \"\"\"\n",
    "            前面已经求出了最后一层的 偏C/偏b 和 偏C/偏w\n",
    "            从倒数第二层开始往前遍历到 正数第二层\n",
    "        \"\"\"\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]              # 第一次遍历时，l = 2 ,表示倒数第二层 的激活函数的输入 z 向量\n",
    "            sp = sigmoid_prime(z)   # 激活函数的倒数\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp     # delta = 后一层的w · 后一层的delta * 当前层的激活函数的倒数\n",
    "            nabla_b[-l] = delta        # 误差、梯度、偏导数\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())     # 偏C/偏w = 误差delta · 前一层的激活层 a 的转秩\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations:np.ndarray, y:np.ndarray):\n",
    "        \"\"\"返回输出层层的误差。公式为\n",
    "            误差 delta = 偏C_x / 偏a = 每层的激活值 - 真实值\n",
    "            output_activations : (10, 1) - (10, 1) numpy.ndarray 类型\n",
    "        \"\"\"\n",
    "        return (output_activations - y)\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"激活函数：sigmoid\"\"\"\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function\n",
    "        Sigmoid 函数的偏导数\n",
    "        直接查资料得到偏导数结果，推导可查看相关书籍\n",
    "    \"\"\"\n",
    "    return sigmoid(z) * (1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaef00a",
   "metadata": {},
   "source": [
    "test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7008f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import network\n",
    "import mnist_loader\n",
    "\n",
    "\n",
    "nn = network.Network([2, 3, 2])         #  实例化对象\n",
    "\n",
    "# 加载 .gz 数据集\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "# 定义3层神经网络，每层神经元个数分别为 784, 30, 10\n",
    "net = network.Network([784, 30, 10])\n",
    "\n",
    "# training_data = list(training_data)\n",
    "\n",
    "# [((784 1),(10, 1)), ((784 1),(10, 1)), ...]\n",
    "# for X, y in training_data:\n",
    "#     print(X.shape, y.shape)\n",
    "#     nabla_b, nabla_w = net.backprop(X, y)\n",
    "#     break\n",
    "\n",
    "# for b, w in zip(nabla_b, nabla_w):\n",
    "#     print(nabla_b[0].shape, nabla_w[0].shape)\n",
    "#     break\n",
    "\n",
    "# [((784 1), 标签), ((784 1), 标签), ...]\n",
    "# test_data = list(test_data)\n",
    "# for X, y in test_data:\n",
    "#     print(X.shape, y.shape)\n",
    "#     print(y)\n",
    "#     break\n",
    "\n",
    "\n",
    "# for w, b in zip(net.weights, net.biases):\n",
    "#     print(w.shape, b.shape)\n",
    "# net.SGD(training_data, 30, 10, 3.0, test_data=test_data)\n",
    "\n",
    "# for X, y in training_data:\n",
    "#     a = net.feedforward(X)\n",
    "#     print(a)\n",
    "#     break\n",
    "\n",
    "net.SGD(training_data, epochs=30, mini_batch_size=10, eta=3.0, test_data=test_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
